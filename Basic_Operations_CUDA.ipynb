{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic_Operations_CUDA.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPsNXxUrFgoumKooTRyvOs4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dyfox100/CUDA-Tutorials/blob/main/Basic_Operations_CUDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVtqLvSvUZsx"
      },
      "source": [
        "### First Switch to GPU Instance\n",
        "\n",
        "Double check to see if the CUDA compiler is installed and updated. The !(bang) operator in jupyter notebooks runs shell commands."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXOBPs-1NIj9",
        "outputId": "51d6f16e-36fd-430a-ff84-af8589c25b0d"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8q__nulU0Hi"
      },
      "source": [
        "Installs the nvcc_plugin needed to run CUDA C/C++ from notebooks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cThW-266NOje",
        "outputId": "6c81b262-55e5-4ab7-fe39-0f689f737008"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-p2jusza2\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-p2jusza2\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4308 sha256=404da5f09ce403dd8c0e4916178172a0cd29b1ff15e307761e303df50e560c6d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nxz9c7q5/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMq6bCrYU6_S"
      },
      "source": [
        "Starts extension running in jupyter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcAqEBx2QHfj",
        "outputId": "6f86c17c-8101-417d-bbb4-cdac687062a1"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAJDi96XU70C"
      },
      "source": [
        "Simple program to make sure the C/C++ CUDA extension works. This won't run on gpu, but if the extension isn't working, colab will try to run this in python and it will blow up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2aRUAcuWQUEO",
        "outputId": "2fa5aafc-03b4-4d0a-ff24-8914fc1460ef"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "\n",
        "/*just to check if the extension is working. None of his runs on the gpu.*/\n",
        "int main() {\n",
        "    printf(\"If this prints, the CUDA etension works!\\n\");\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "If this prints, the CUDA etension works!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltVfWiZ9Dpay"
      },
      "source": [
        "# Terminology\n",
        "\n",
        "### Host vs Device\n",
        "\n",
        "Host -- The cpu that runs starts the kernel\n",
        "\n",
        "Device -- GPUs or other computation devices that run the kernel.\n",
        "\n",
        "Memory is normally listed as host or device memory.\n",
        "\n",
        "### Grids and Blocks\n",
        "\n",
        "Grid -- the entire space of threads in a kernel. Organized into blocks.\n",
        "\n",
        "Block -- Threads are organized into blocks. Inside a block, threads have shared memory and can use synchronization operations. Outside a block, the kernel must be restarted to synchronize or communicate between different blocks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx9L3_99OVa-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c64939e-2320-47aa-c03f-f9dd82697387"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void hello_cuda() {\n",
        "    printf(\"Hello from CUDA!\\n\");\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // kernel launch params. First is num of blocks. Second is num threads in block.\n",
        "    // Should print 6 times, 3 threads per block on 2 blocks.\n",
        "    hello_cuda <<<2, 3>>>();\n",
        "    \n",
        "    // Waits until kernel completes. Necessary because main function will finish\n",
        "    // before the kernel prints otherwise.\n",
        "    cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello from CUDA!\n",
            "Hello from CUDA!\n",
            "Hello from CUDA!\n",
            "Hello from CUDA!\n",
            "Hello from CUDA!\n",
            "Hello from CUDA!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTnjN_SwEUBD"
      },
      "source": [
        "### ID Numbers and Dim3 Coordinates\n",
        "\n",
        "The variables threadIdx, blockIdx, blockDim, and gridDim can provide us with id numbers and dimensions for the gird, blocks, and threads. \n",
        "\n",
        "The grid and each block ar organized into a three dimensional coordinate system.\n",
        "There is a struct in CUDA C/C++ that can be used to specify these three dimensional shapes. It's called dim3.\n",
        "\n",
        "If using dim3 / (x, y, z) coordinates all coords must be > 0. \n",
        "\n",
        "There must be less than 1024 threads in x,y and 64 threads in z. And x * y * z must be less than 1024.\n",
        "\n",
        "Must be less than 65536 thread blocks in y and z dirs and 2^32 - 1 in x .\n",
        "\n",
        "Note that each thread runs independently, so the output is intermingled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iKc3mT8eZfk",
        "outputId": "f1c3d8bf-17af-4288-c992-63e89a6f1050"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void print_thread_id() {\n",
        "    // Kernels have access to threadIDx structs that identify threads in a block.\n",
        "    printf(\"Thread ID is: (%d, %d, %d)\\n\", threadIdx.x, threadIdx.y, threadIdx.z);\n",
        "    // Also have access to blockIdx which identifys blocks in the grid.\n",
        "    printf(\"Block ID is: (%d, %d, %d)\\n\", blockIdx.x, blockIdx.y, blockIdx.z);\n",
        "    // blockDim structs hold the number of threads in each block. Same for all\n",
        "    // blocks / threads.\n",
        "    printf(\"Each block has %d by %d by %d blocks.\\n\",\n",
        "           blockDim.x, blockDim.y, blockDim.z);\n",
        "    // There is also a gridDim struct which gives dimensions of the grid (in number of blocks).\n",
        "    printf(\"The grid has %d by %d by %d blocks.\\n\", \n",
        "           gridDim.x, gridDim.y, gridDim.z);\n",
        "    // We can use this info to get the total number of threads.\n",
        "    printf(\"The total number of threads is: %d.\\n\", \n",
        "           (blockDim.x * blockDim.y * blockDim.z) * (gridDim.x * gridDim.y * gridDim.z));\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    dim3 block(2, 1, 1);\n",
        "    dim3 grid(2, 2, 1);\n",
        "    \n",
        "    print_thread_id <<<grid, block>>>();\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thread ID is: (0, 0, 0)\n",
            "Thread ID is: (1, 0, 0)\n",
            "Thread ID is: (0, 0, 0)\n",
            "Thread ID is: (1, 0, 0)\n",
            "Thread ID is: (0, 0, 0)\n",
            "Thread ID is: (1, 0, 0)\n",
            "Thread ID is: (0, 0, 0)\n",
            "Thread ID is: (1, 0, 0)\n",
            "Block ID is: (1, 1, 0)\n",
            "Block ID is: (1, 1, 0)\n",
            "Block ID is: (0, 0, 0)\n",
            "Block ID is: (0, 0, 0)\n",
            "Block ID is: (1, 0, 0)\n",
            "Block ID is: (1, 0, 0)\n",
            "Block ID is: (0, 1, 0)\n",
            "Block ID is: (0, 1, 0)\n",
            "Each block has 2 by 1 by 1 blocks.\n",
            "Each block has 2 by 1 by 1 blocks.\n",
            "Each block has 2 by 1 by 1 blocks.\n",
            "Each block has 2 by 1 by 1 blocks.\n",
            "Each block has 2 by 1 by 1 blocks.\n",
            "Each block has 2 by 1 by 1 blocks.\n",
            "Each block has 2 by 1 by 1 blocks.\n",
            "Each block has 2 by 1 by 1 blocks.\n",
            "The grid has 2 by 2 by 1 blocks.\n",
            "The grid has 2 by 2 by 1 blocks.\n",
            "The grid has 2 by 2 by 1 blocks.\n",
            "The grid has 2 by 2 by 1 blocks.\n",
            "The grid has 2 by 2 by 1 blocks.\n",
            "The grid has 2 by 2 by 1 blocks.\n",
            "The grid has 2 by 2 by 1 blocks.\n",
            "The grid has 2 by 2 by 1 blocks.\n",
            "The total number of threads is: 8.\n",
            "The total number of threads is: 8.\n",
            "The total number of threads is: 8.\n",
            "The total number of threads is: 8.\n",
            "The total number of threads is: 8.\n",
            "The total number of threads is: 8.\n",
            "The total number of threads is: 8.\n",
            "The total number of threads is: 8.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtwMAG5l82AJ"
      },
      "source": [
        "### Getting a unique thread index\n",
        "To get a unique index for each thread:\n",
        "1. Create a unique block index.\n",
        "* Multiply the first dimension of the block id by the number of blocks in the other two dimensions.\n",
        "* Add that quantity to the second block id dimension multiplied by the number of blocks in the third dimension.\n",
        "* Add the third block id dimension to the previous quantity.\n",
        "\n",
        "\n",
        "2. Create starting point for unique thread id.\n",
        "* Multiply the unique block id by the numbers of threads in all dimensions.\n",
        "\n",
        "3. Create a unique thread index.\n",
        "* Multiply the thread id in the first dimension by the number of threads in the otehr two dims.\n",
        "* Add that to the second dimension multiplied by the number of threads in the third.\n",
        "* Add the third dimension to that quantity.\n",
        "\n",
        "4. Add the quantiy in step 2 to step 3.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQBLcq2Af1hW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "793add45-9257-419f-8cf1-1ecb37c4c877"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void print_unique_index() {\n",
        "    int uniqueBlock = blockIdx.x * gridDim.y * gridDim.z;\n",
        "    uniqueBlock += blockIdx.y * gridDim.z;\n",
        "    uniqueBlock += blockIdx.z;\n",
        "\n",
        "    int blockOffset = uniqueBlock * blockDim.x * blockDim.y * blockDim.z;\n",
        "\n",
        "    int thread = threadIdx.x * blockDim.y * blockDim.z;\n",
        "    thread += threadIdx.y * blockDim.z;\n",
        "    thread += threadIdx.z;\n",
        "\n",
        "    int uniqueThreadID = blockOffset + thread;\n",
        "\n",
        "    printf(\"%d\\n\", uniqueThreadID);\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "int main() {\n",
        "    dim3 grid (2, 1, 1);\n",
        "    dim3 block (2, 2, 3);\n",
        "    print_unique_index <<<grid, block>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "}"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "6\n",
            "3\n",
            "9\n",
            "1\n",
            "7\n",
            "4\n",
            "10\n",
            "2\n",
            "8\n",
            "5\n",
            "11\n",
            "12\n",
            "18\n",
            "15\n",
            "21\n",
            "13\n",
            "19\n",
            "16\n",
            "22\n",
            "14\n",
            "20\n",
            "17\n",
            "23\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89aC-2XdYcVU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk_7LZEp8aqj"
      },
      "source": [
        "### Copying Memory to a Device and Back\n",
        "1. Allocate memory on the host using a regular malloc.\n",
        "2. Fill that memory with whatever you're trying to use on the gpu/device.\n",
        "3. Allocate the same amount of memory on the device using cudaMalloc.\n",
        "4. Use cudaMemcpy to copy memory from the host to the device.\n",
        "5. Perform your computation on the device.\n",
        "6. Use cudaMemcpy to copy the memory back to your host.\n",
        "7. Free the memory on the device using cudaFree.\n",
        "\n",
        "\n",
        "### Parameters\n",
        "Paramters can be sent to the kernel by passing them in the function call.\n",
        "\n",
        "They are passed by value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnfbUoQyU8gx"
      },
      "source": [
        "This is a basic kernel that copies two integers to the device, passes pointers to those integers to the device kernel, adds them, and copies them back. This only runs on one thread, so it's not really taking advantage of the gpu. It's just a demonstration of how to copy things."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1LOgyrbOPn3",
        "outputId": "97dfaaf9-798c-40f6-9f84-413d36688b8f"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "\n",
        "// Simple gpu function to add two variables.\n",
        "__global__ void add(int *a, int *b, int *r){\n",
        "    *r = *a + *b;\n",
        "}\n",
        "\n",
        "// Main function to run the gpu code.\n",
        "int main() { \n",
        "    int a, b, r;\n",
        "\n",
        "    // Device copies.\n",
        "    int *d_a, *d_b, *d_r;\n",
        "\n",
        "    // Allocates space on device for the three ints.\n",
        "    // Puts pointers to this space in the variables d_a, d_b, d_r.\n",
        "    cudaMalloc((void **)&d_a, sizeof(int));\n",
        "    cudaMalloc((void **)&d_b, sizeof(int));\n",
        "    cudaMalloc((void **)&d_r, sizeof(int));\n",
        "\n",
        "   a = 2;\n",
        "   b = 5;\n",
        "\n",
        "    // Copy variables to device.\n",
        "    cudaMemcpy(d_a, &a, sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, &b, sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel on the device.\n",
        "    add<<<1,1>>>(d_a, d_b, d_r);\n",
        "\n",
        "    // Copy the result back to the host and check for errors in copy.\n",
        "    cudaError err = cudaMemcpy(&r, d_r, sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Check for errors. Err holds the error code, and cudaSuccess holds the expected code.\n",
        "    if(err!=cudaSuccess) {\n",
        "        printf(\"Error copying to Host: %s\\n\", cudaGetErrorString(err));\n",
        "    }\n",
        "\n",
        "    printf(\"Adding %d with %d on the gpu yields %d\\n\",a, b, r);\n",
        "\n",
        "    // Need to free memory on the device.\n",
        "    cudaFree(d_a);\n",
        "    cudaFree(d_b);\n",
        "    cudaFree(d_r);\n",
        "\n",
        "    return 0;\n",
        "\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adding 2 with 5 on the gpu yields 7\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdOLdjIFYewo"
      },
      "source": [
        "### Unified Memory Malloc API\n",
        "CUDA also offers a simple api to allocate space on both the host and the device. Below is the add kernel with the host program changed to use the unified memory api."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDaNsNCTYzFK",
        "outputId": "3252c785-87d9-4398-98ea-4fae58262b5c"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "\n",
        "// Simple gpu function to add two variables.\n",
        "__global__ void add(int *a, int *b, int *r){\n",
        "    *r = *a + *b;\n",
        "}\n",
        "\n",
        "// Main function to run the gpu code.\n",
        "int main() { \n",
        "    int *a, *b,*r;\n",
        "\n",
        "    // Allocates space on device and host for the three ints.\n",
        "    cudaMallocManaged((void **)&a, sizeof(int));\n",
        "    cudaMallocManaged((void **)&b, sizeof(int));\n",
        "    cudaMallocManaged((void **)&r, sizeof(int));\n",
        "\n",
        "   *a = 2;\n",
        "   *b = 5;\n",
        "   *r = 0;\n",
        "\n",
        "    // Launch kernel on the device.\n",
        "    add<<<1,1>>>(a, b, r);\n",
        "\n",
        "    // Now we need to synchornize. The memcopy did this by default\n",
        "    // in the last kernel since it waits for the current computation to finish.\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    printf(\"Adding %d with %d on the gpu yields %d\\n\", *a, *b, *r);\n",
        "\n",
        "    // Need to free memory on the device.\n",
        "    cudaFree(a);\n",
        "    cudaFree(b);\n",
        "    cudaFree(r);\n",
        "\n",
        "    return 0;\n",
        "\n",
        "}"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Adding 2 with 5 on the gpu yields 7\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMDp1Q88_41o"
      },
      "source": [
        "### Querying Device Properties\n",
        "CUDA provides an apit to query a devices properties. In paticular, this can be used to make sure we don't exceed any of the limits on the number of threads, or to check the warp size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Uf6LBBw_3OF",
        "outputId": "1348b59b-08b0-4e72-c079-d543da532e32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        " \n",
        "// Print device properties\n",
        "void printDevProp(cudaDeviceProp devProp)\n",
        "{\n",
        "    printf(\"Major number:                  %d\\n\",  devProp.major);\n",
        "    printf(\"Minor number:                  %d\\n\",  devProp.minor);\n",
        "    printf(\"Name:                          %s\\n\",  devProp.name);\n",
        "    printf(\"Total global memory:           %u\\n\",  devProp.totalGlobalMem);\n",
        "    printf(\"Total shared memory per block: %u\\n\",  devProp.sharedMemPerBlock);\n",
        "    printf(\"Total registers per block:     %d\\n\",  devProp.regsPerBlock);\n",
        "    printf(\"Warp size:                     %d\\n\",  devProp.warpSize);\n",
        "    printf(\"Maximum memory pitch:          %u\\n\",  devProp.memPitch);\n",
        "    printf(\"Maximum threads per block:     %d\\n\",  devProp.maxThreadsPerBlock);\n",
        "    for (int i = 0; i < 3; ++i)\n",
        "    printf(\"Maximum dimension %d of block:  %d\\n\", i, devProp.maxThreadsDim[i]);\n",
        "    for (int i = 0; i < 3; ++i)\n",
        "    printf(\"Maximum dimension %d of grid:   %d\\n\", i, devProp.maxGridSize[i]);\n",
        "    printf(\"Clock rate:                    %d\\n\",  devProp.clockRate);\n",
        "    printf(\"Total constant memory:         %u\\n\",  devProp.totalConstMem);\n",
        "    printf(\"Texture alignment:             %u\\n\",  devProp.textureAlignment);\n",
        "    printf(\"Concurrent copy and execution: %s\\n\",  (devProp.deviceOverlap ? \"Yes\" : \"No\"));\n",
        "    printf(\"Number of multiprocessors:     %d\\n\",  devProp.multiProcessorCount);\n",
        "    printf(\"Kernel execution timeout:      %s\\n\",  (devProp.kernelExecTimeoutEnabled ? \"Yes\" : \"No\"));\n",
        "    return;\n",
        "}\n",
        " \n",
        "int main()\n",
        "{\n",
        "    // Number of CUDA devices\n",
        "    int devCount;\n",
        "    cudaGetDeviceCount(&devCount);\n",
        "    printf(\"CUDA Device Query...\\n\");\n",
        "    printf(\"There are %d CUDA devices.\\n\", devCount);\n",
        " \n",
        "    // Iterate through devices\n",
        "    for (int i = 0; i < devCount; ++i)\n",
        "    {\n",
        "        // Get device properties\n",
        "        printf(\"\\nCUDA Device #%d\\n\", i);\n",
        "        cudaDeviceProp devProp;\n",
        "        cudaGetDeviceProperties(&devProp, i);\n",
        "        printDevProp(devProp);\n",
        "    }\n",
        " \n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Device Query...\n",
            "There are 1 CUDA devices.\n",
            "\n",
            "CUDA Device #0\n",
            "Major number:                  3\n",
            "Minor number:                  7\n",
            "Name:                          Tesla K80\n",
            "Total global memory:           3407020032\n",
            "Total shared memory per block: 49152\n",
            "Total registers per block:     65536\n",
            "Warp size:                     32\n",
            "Maximum memory pitch:          2147483647\n",
            "Maximum threads per block:     1024\n",
            "Maximum dimension 0 of block:  1024\n",
            "Maximum dimension 1 of block:  1024\n",
            "Maximum dimension 2 of block:  64\n",
            "Maximum dimension 0 of grid:   2147483647\n",
            "Maximum dimension 1 of grid:   65535\n",
            "Maximum dimension 2 of grid:   65535\n",
            "Clock rate:                    823500\n",
            "Total constant memory:         65536\n",
            "Texture alignment:             512\n",
            "Concurrent copy and execution: Yes\n",
            "Number of multiprocessors:     13\n",
            "Kernel execution timeout:      No\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG-ne573EibO"
      },
      "source": [
        "### Warps\n",
        "A warp is a unit of threads that are executed simultaneously. Basically one of the streaming multi processors can execute the warp size of threads at a time. We may want to keep the thread block size a multiple of 32 so that we avoid wasting cycles at the end of each thread block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4Go6OsFAOZ-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}