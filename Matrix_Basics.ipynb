{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Matrix_Basics",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "https://github.com/Dyfox100/CUDA-Tutorials/blob/main/Matrix_Basics.ipynb",
      "authorship_tag": "ABX9TyPy2NV9fAEgLOLj29XzrxRa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dyfox100/CUDA-Tutorials/blob/main/Matrix_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfATpI3ccbLy"
      },
      "source": [
        "# Matrix Operations in CUDA\n",
        "\n",
        "### But first, set up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4toplTmEcWby",
        "outputId": "58c93585-8cae-43b8-d55b-a749757a5a5e"
      },
      "source": [
        "!nvcc --version\n",
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "Cuda compilation tools, release 10.1, V10.1.243\n",
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-35fj8uiy\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-35fj8uiy\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.2-cp36-none-any.whl size=4308 sha256=bd407b9e4d880bac2236731ae0e2037949be4e353e8f6dddc8da1d39618d1e16\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2xn_dqhj/wheels/10/c2/05/ca241da37bff77d60d31a9174f988109c61ba989e4d4650516\n",
            "Successfully built NVCCPlugin\n",
            "Installing collected packages: NVCCPlugin\n",
            "Successfully installed NVCCPlugin-0.0.2\n",
            "created output directory at /content/src\n",
            "Out bin /content/result.out\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5su2Ntmhc8Xt"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri7BysTKhi7I"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6lO2LlFdxRM"
      },
      "source": [
        "### Adding Two Vectors\n",
        "\n",
        "In this next piece of code, we'll add two vectors. \n",
        "\n",
        "In general, we'd like each thread to only work on one piece of data, but in this example we're going to write a loop. \n",
        "\n",
        "Adding a loop ensures we can process all of the elements if the number of data points exceeds the number of threads that we can run on the device.\n",
        "\n",
        "This uses a 1 dimensional grid, 1 dimensional blocks, and a grid stride loop. \n",
        "\n",
        "Grid stride means that each thread operates on one element then adds the total number of threads in the grid to get the index of the next element. \n",
        "\n",
        "We'll use a multiple of 32 for the block size to avoid wasting time at the end of each block. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1iybppRd270",
        "outputId": "ddbd5b58-ede4-4150-8372-56b40231790b"
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "__global__ void add(int size, float *x, float *y) {\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int stride = blockDim.x * gridDim.x;\n",
        "\n",
        "    for (int i = index; i < size; i += stride) {\n",
        "        y[i] = x[i] + y[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "    int size = 1000000000;\n",
        "    float *x, *y;\n",
        "\n",
        "    // Allocate space for both the vectors on both the host and device.\n",
        "    cudaMallocManaged(&x, size*sizeof(float));\n",
        "    cudaMallocManaged(&y, size*sizeof(float));\n",
        "\n",
        "    for (int i = 0; i < size; i++) {\n",
        "        x[i] = 2.0f;\n",
        "        y[i] = -1.0f;\n",
        "    }\n",
        "\n",
        "    // Launch kernel with 16 blocks with 512 threads in each block.\n",
        "    // 512 / 32 == 16, so the block size is a multiple of 32. \n",
        "    // This is 8192 threads, so the loop is necessary.\n",
        "    add<<<16, 512>>>(size, x, y);\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    for(int i = 0; i < size; i++) {\n",
        "        if (abs(y[i] - 1.0f) > 0.001f) {\n",
        "            printf(\"Error is greater than 0.001! Value is: %f\", y[i]);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Now we can start a very large number of threads so the loop isn't really \n",
        "    // necessary. Note 1024 is the maximum number of threads in a block.\n",
        "    add<<<1048576, 1024>>>(size, x, y);\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    for(int i = 0; i < size; i++) {\n",
        "        if (abs(y[i] - 3.0f) > 0.001f) {\n",
        "            printf(\"Error is greater than 0.001! Value is: %f\", y[i]);\n",
        "        }\n",
        "    }\n",
        "    printf(\"Done! No errors detected!\\n\");\n",
        "    printf(\"First value in y is: %f and should be 3.0\\n\", y[0]);\n",
        "    printf(\"Wow that was quick. We just added a billion floating point numbers twice!\\n\");\n",
        "    cudaFree(x);\n",
        "    cudaFree(y);\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done! No errors detected!\n",
            "First value in y is: 3.000000 and should be 3.0\n",
            "Wow that was quick. We just added a billion floating point numbers twice!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS8w8QNBJ7eZ"
      },
      "source": [
        "### Dot Product\n",
        "To calculate the dot product we need to multiply wach pair of elements in two vectors, and sum them.\n",
        "\n",
        "To do this, we'll use shared memory on a thread block to calculate a local sum,\n",
        "then reduce over all thread blocks.\n",
        "\n",
        "This will require a few new things. First off we'll need to use shared memory, secondly, we'll need to synchronize threads, and third, we'll need to use atomic operations. \n",
        "\n",
        "Each thread block has a piece of fast shared memory associated with it. This shared memory segment is only accessible from inside each thread block. \n",
        "\n",
        "To calculate the dot product on on thread block's id's, we'll create a shared memory segment to hold all of the individual products. This can be done using the \\__shared__ key word.\n",
        "\n",
        "Next we'll reduce / sum the values in each thread block's array of products. But first we'll need to synchronize the thread block. If each thread in the thread block isn't done adding their product to the shared array, we will get weird results.\n",
        "\n",
        "Waiting for all threads to get to a certain point inside a thread block can be accomplished by using the \\__syncthreads function.\n",
        "\n",
        "Next we'll actually do the reduction to get each thread blocks portion of the dot product. But we now need to add all of the thread blocks portions together. We cna just add them to the result variable, but we need to be careful. Thread blocks run concurrently, so imagine what happens if thread block 4 reads the result variable, then thread block 5 writes to the thread block variable, then thread block 4 does it's add and writes to the result variable. The result would completly miss the contribution from block 5 becuase it wasn't there when block 4 read the result variable!\n",
        "\n",
        "To fix this we can use an atomic operation. Atomic operations can read a memory location, perform some operation, then write back to the memory location without having anyone else read / write to that memory location. We'll use an atomicAdd here.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6MN98hqJ6id",
        "outputId": "9ea2361b-a27a-4d3a-e475-b595bb756de8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%cu\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "#define SIZE 1000000000\n",
        "#define THREADS_PER_BLOCK 1024\n",
        "\n",
        "__global__ void dot(float *x, float *y, float *result) {\n",
        "    __shared__ float blockProducts[THREADS_PER_BLOCK];\n",
        "\n",
        "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    blockProducts[threadIdx.x] = x[index] * y[index];\n",
        "\n",
        "    __syncthreads();\n",
        "\n",
        "    \n",
        "    if (threadIdx.x == 0) {\n",
        "        float blockSum = 0.0;\n",
        "        for (int i = 0; i < THREADS_PER_BLOCK; i++) {\n",
        "            blockSum += blockProducts[i];\n",
        "        }\n",
        "        atomicAdd(result, blockSum);\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "    float *x, *y, *result;\n",
        "\n",
        "    // Allocate space for both the vectors on both the host and device.\n",
        "    cudaMallocManaged(&x, SIZE*sizeof(float));\n",
        "    cudaMallocManaged(&y, SIZE*sizeof(float));\n",
        "\n",
        "    // Allocate space for the result.\n",
        "    cudaMallocManaged(&result, sizeof(float));\n",
        "\n",
        "    for (int i = 0; i < SIZE; i++) {\n",
        "        x[i] = 1.0f;\n",
        "        y[i] = -1.0f;\n",
        "    }\n",
        "\n",
        "    int blocks = (SIZE + (THREADS_PER_BLOCK - 1)) / THREADS_PER_BLOCK;\n",
        "    dot <<< blocks, THREADS_PER_BLOCK >>>(x, y, result);\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    if (abs(*result + SIZE) > 1.0) {\n",
        "        printf(\"Error in dot product, dot product is: %f\", *result);\n",
        "    }\n",
        "    else {\n",
        "      printf(\"Done! No errors detected! Dot product is: %f\\n\", *result);  \n",
        "    }\n",
        "    \n",
        "    cudaFree(x);\n",
        "    cudaFree(y);\n",
        "    cudaFree(result);\n",
        "    return 0;\n",
        "}\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done! No errors detected! Dot product is: -1000000000.000000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaDjrrS6Nva3",
        "outputId": "ad8f8467-4b1f-4d1b-fb36-10c3b4061411",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!nvcc  ./drive/MyDrive/test.cpp -I/usr/include/opencv -lopencv_core -lopencv_highgui -lopencv_imgproc -lopencv_imgcodecs -lopencv_core \n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[01m\u001b[K./drive/MyDrive/test.cpp:9:1:\u001b[m\u001b[K \u001b[01;31m\u001b[Kerror: \u001b[m\u001b[K‘\u001b[01m\u001b[K__global__\u001b[m\u001b[K’ does not name a type; did you mean ‘\u001b[01m\u001b[K__locale_t\u001b[m\u001b[K’?\n",
            " \u001b[01;31m\u001b[K__global__\u001b[m\u001b[K void brighten()\n",
            " \u001b[01;31m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n",
            " \u001b[32m\u001b[K__locale_t\u001b[m\u001b[K\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKvequ4swq4b",
        "outputId": "658f1031-28be-432b-872b-6211ea020141"
      },
      "source": [
        "!./a.out\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting!\n",
            "The first value in the r array is: 76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz2fg1nn1lTV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}